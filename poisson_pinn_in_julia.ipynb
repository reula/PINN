{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Network (PINN) in Julia\n",
    "\n",
    "This is a simple showcase of how PINNs can learn the solution to (Partial) Differential Equations without labelled data by leveraging automatic differentation to train on a residuum loss of the boundary value problem.\n",
    "\n",
    "We will consider the 1D Poisson equation\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial^2 u}{\\partial x^2} &= - f(x), \\qquad & x \\in \\Omega = (0, 1)\n",
    "\\\\\n",
    "u(0) &= 0 = u(1)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For $f(x) = \\sin(\\pi x)$, the analytical solution is $\\hat{u}(x) = \\frac{1}{\\pi^2} \\sin(\\pi x)$. We aim to train a shallow neural network to learn the mapping $x \\mapsto u$.\n",
    "\n",
    "This can be done by chosing collocation points, i.e., random points within the domain at which we enfore the PDE. A difference of the neural network against this underlying description will constitute to the loss. Additionally, we will also penalize if the network does not obey the homogeneous Dirichlet boundary conditions. As such our loss is\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\alpha_{int} \\frac{1}{2N} \\sum_{i=1}^N \\left( \\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{x_i} + f(x_i)  \\right)^2  + \\alpha_{bc} \\frac{1}{2 \\cdot 2} \\left( u(0)^2 + u(1)^2 \\right)\n",
    "$$\n",
    "\n",
    "with hyperparameters to weigh the two components of the loss. Our neural network is parameterized by its weights and biases. We can backprop from this loss into the parameter space to obtain a gradient estimate which guides a gradient-based optimizer (here we will use ADAM).\n",
    "\n",
    "**Julia's reverse-mode automatic differentiation ecosystem, as of now, does not properly support higher-order autodiff**. However, this is crucial, because ultimately we need three autodiff passes, two to get the second derivative within the loss formulation and another one to obtain a gradient estimate in the parameter space.\n",
    "\n",
    "## Employed architecture\n",
    "\n",
    "This intro follows the work of Lagaris et al. ([https://arxiv.org/abs/physics/9705023](https://arxiv.org/abs/physics/9705023)) that use a neural network with **one hidden layer**. By the universal approximation theorem, this is sufficient to capture (almost) any function. Hence, given the hidden layer is chosen large enough, it should also be possible to approximate the solution to the PDE. The forward pass of the network becomes\n",
    "\n",
    "$$\n",
    "u = v^T \\sigma.(w x + b)\n",
    "$$\n",
    "\n",
    "We assume our network to be a scalar-to-scalar map, hence\n",
    "\n",
    "$$\n",
    "x \\in \\R, w \\in \\R^h, b \\in \\R^h, v \\in \\R^h, u \\in \\R\n",
    "$$\n",
    "\n",
    "with $h$ being the size of the hidden dimension.\n",
    "\n",
    "### Analytical Input-Output derivative\n",
    "\n",
    "**Our goal is to reduce to only one application of the Julia reverse-mode autodiff engine; the pullback from loss to parameter space.** As such, we want to find hand-coded derivatives for the network architecture in their input-output relation.\n",
    "\n",
    "We can derive this simple shallow network to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial u}{\\partial x} &= (v \\odot w)^T \\sigma'(w x + b)\n",
    "\\\\\n",
    "\\frac{\\partial^2 u}{\\partial x^2} &= (v \\odot w.^2)^T \\sigma''(w x + b)\n",
    "\\\\\n",
    "\\frac{\\partial^l u}{\\partial x^l} &= (v \\odot w.^l)^T \\sigma^{(l)}(w x + b)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### The batched case\n",
    "\n",
    "For all practical applications, we want to query our network batched, i.e., for multiple inputs at the same time. Following the Julia convention, we will therefore the denote the collection of inputs as $x \\in \\R^{1 \\times N}$ and the collection of outputs as $u \\in \\R^{1 \\times N}$. As such, the forward pass becomes\n",
    "\n",
    "$$\n",
    "u = V \\cdot \\sigma.(W \\cdot x .+ b)\n",
    "$$\n",
    "\n",
    "with the sizes\n",
    "\n",
    "$$\n",
    "x \\in \\R^{1 \\times N}, W \\in \\R^{h, 1}, b \\in \\R^h, V \\in \\R^{1, h}, u \\in \\R^{1 \\times N}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the sigmoid\n",
    "\n",
    "Its higher derivatives can be expressed using the primal output\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\n",
    "\\\\\n",
    "\\sigma' &= \\sigma (1 - \\sigma)\n",
    "\\\\\n",
    "\\sigma'' &= \\sigma (1 - \\sigma) \\left( 1- 2\\sigma \\right) = \\sigma' \\left( 1- 2\\sigma \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optimisers, Zygote, Plots, Random, Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_collocation_points = 50\n",
    "HIDDEN_DEPTH = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "N_EPOCHS = 40_000\n",
    "BC_LOSS_WEIGHT = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs_function(x) = sin(π * x)\n",
    "analytical_solution(x) = sin(π * x) / π^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(x) = 1.0 / (1.0 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_parameters()\n",
    "    # Initialize the weights according to the Xavier Glorot initializer\n",
    "    uniform_limit = sqrt(6 / (1 + HIDDEN_DEPTH))\n",
    "    W = rand(\n",
    "        rng,\n",
    "        Uniform(-uniform_limit, +uniform_limit),\n",
    "        HIDDEN_DEPTH,\n",
    "        1,\n",
    "    )\n",
    "    V = rand(\n",
    "        rng,\n",
    "        Uniform(-uniform_limit, +uniform_limit),\n",
    "        1,\n",
    "        HIDDEN_DEPTH,\n",
    "    )\n",
    "    b = zeros(HIDDEN_DEPTH)\n",
    "    parameters = (; W, V, b)\n",
    "    return parameters\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = (; W, V, b)\n",
    "methds = [:log10, :direct]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_forward(x, p) = p.V * sigmoid.(p.W * x .+ p.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = reshape(collect(range(0.0, stop=1.0, length=100)), (1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot initial prediction of the network (together with the analytical solution)\n",
    "plot(x_line[:], network_forward(x_line, parameters)[:], label=\"initial prediction\")\n",
    "plot!(x_line[:], analytical_solution.(x_line[:]), label=\"analytical_solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function network_output_and_first_two_derivatives(x, p)\n",
    "    activated_state = sigmoid.(p.W * x .+ p.b)\n",
    "    sigmoid_prime = activated_state .* (1.0 .- activated_state)\n",
    "    sigmoid_double_prime = sigmoid_prime .* (1.0 .- 2.0 .* activated_state)\n",
    "\n",
    "    output = p.V * activated_state\n",
    "    first_derivative = (p.V .* p.W') * sigmoid_prime\n",
    "    second_derivative = (p.V .* p.W' .* p.W') * sigmoid_double_prime\n",
    "\n",
    "    return output, first_derivative, second_derivative\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output, _first_derivative, _second_derivative = network_output_and_first_two_derivatives(x_line, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_first_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_zygote_first_derivative = Zygote.gradient(x -> sum(network_forward(x, parameters)), x_line)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interior_collocation_points = rand(rng, Uniform(0.0, 1.0), (1, N_collocation_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_collocation_points = [0.0 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_forward_direct(p)\n",
    "    output, first_derivative, second_derivative = network_output_and_first_two_derivatives(\n",
    "        interior_collocation_points,\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    interior_residuals = second_derivative .+ rhs_function.(interior_collocation_points)\n",
    "\n",
    "    interior_loss = 0.5 * mean(interior_residuals.^2)\n",
    "\n",
    "    boundary_residuals = network_forward(boundary_collocation_points, p) .- 0.0\n",
    "\n",
    "    boundary_loss = 0.5 * mean(boundary_residuals.^2)\n",
    "\n",
    "    total_loss = interior_loss + BC_LOSS_WEIGHT * boundary_loss\n",
    "    return total_loss\n",
    "end\n",
    "\n",
    "function loss_forward_log10(p)\n",
    "    output, first_derivative, second_derivative = network_output_and_first_two_derivatives(\n",
    "        interior_collocation_points,\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    interior_residuals = second_derivative .+ rhs_function.(interior_collocation_points)\n",
    "\n",
    "    interior_loss = 0.5 * mean(interior_residuals.^2)\n",
    "\n",
    "    boundary_residuals = network_forward(boundary_collocation_points, p) .- 0.0\n",
    "\n",
    "    boundary_loss = 0.5 * mean(boundary_residuals.^2)\n",
    "\n",
    "    total_loss = interior_loss + BC_LOSS_WEIGHT * boundary_loss\n",
    "    return log10(total_loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_forward_direct(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, back = Zygote.pullback(loss_forward_direct, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back(1.0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history_direct = []\n",
    "loss_history_log10 = []\n",
    "\n",
    "for method in methds\n",
    "    println(\"Training with method: $method\")\n",
    "\n",
    "if method == :direct\n",
    "    parameters = initialize_parameters()\n",
    "    opt_state = Optimisers.setup(opt, parameters)\n",
    "    for i in 1:N_EPOCHS\n",
    "        loss, back = Zygote.pullback(loss_forward_direct, parameters)\n",
    "        push!(loss_history_direct, log10(loss))\n",
    "        grad, = back(1.0)\n",
    "        opt_state, parameters = Optimisers.update(opt_state, parameters, grad)\n",
    "        if i % 100 == 0\n",
    "            println(\"Epoch: $i, Loss: $loss\")\n",
    "        end\n",
    "    end\n",
    "elseif method == :log10\n",
    "    parameters = initialize_parameters()\n",
    "    opt_state = Optimisers.setup(opt, parameters)\n",
    "    for i in 1:N_EPOCHS\n",
    "        loss, back = Zygote.pullback(loss_forward_log10, parameters)\n",
    "        push!(loss_history_log10, loss)\n",
    "        grad, = back(1.0)\n",
    "        opt_state, parameters = Optimisers.update(opt_state, parameters, grad)\n",
    "        if i % 100 == 0\n",
    "            println(\"Epoch: $i, Loss: $loss\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(loss_history_direct, label=\"direct loss function\")#, yscale=:log10)\n",
    "plot!(loss_history_log10, label=\"log10 of loss funciton\")#, yscale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_line[:], network_forward(x_line, parameters)[:], label=\"final prediction\")\n",
    "plot!(x_line[:], analytical_solution.(x_line[:]), label=\"analytical_solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
